#+title: Ist das ein Graph oder kann das weg? Funktionales Deep Learning in Haskell
#+author: Raoul Schlotterbeck
#+REVEAL_PLUGINS: (notes)
#+REVEAL_THEME: ./css/themes/active.css
#+REVEAL_HLEVEL: 100
#+REVEAL_TRANS: none
#+OPTIONS: toc:nil reveal-center:f H:4

* Neuronale Netze

#+REVEAL_HTML: <div style="width: 100%; overflow: hidden;">

#+REVEAL_HTML: <div style="width: 500px; float: left;">
#+begin_src haskell
neuralNet wL bL ... w1 b1 = 
  layer wL bL 
  . ... 
  . layer w1 b1

layer w b =
  fmap activationFunction
  . vectorAddition b
  . matrixVectorProduct w
#+end_src

#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="margin-left: 500px;">
[[./pics/neural_net.jpg]]
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="text-align: center;">
=> Komposition purer Funktionen
#+REVEAL_HTML: </div>

#+REVEAL_HTML: </div>


#+begin_notes
foo
#+end_notes

* Neuronale Netze

#+REVEAL_HTML: <p>
"... Schichten (die im modernen maschinellen Lernen als zustandsbehaftete Funktionen 
mit impliziten Parametern verstanden werden sollten) werden typischerweise als 
Python-Klassen dargestellt, deren Konstruktoren ihre Parameter erzeugen und 
initialisieren..."
#+REVEAL_HTML: </p>

#+REVEAL_HTML: <div style="font-size: 60%;">
#+REVEAL_HTML: <p>
"... layers (which in modern machine learning should really be understood as 
stateful functions with implicit parameters) are typically expressed as Python 
classes whose constructors create and initialize their parameters..."
#+REVEAL_HTML: </p>
#+REVEAL_HTML: <cite style:"font-size=30%">
https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
#+REVEAL_HTML: </cite>
#+REVEAL_HTML: </div>

* Training

- Finde Parameter $\omega_i = (w_i, b_i)$, für die das NN auf einem gegebenen Trainingsdatensatz
  möglichst gute Ergebnisse liefert

- Was gut ist, wird von einer (skalarwertigen) Fehlerfunktion beurteilt

 => Löse Optimierungsproblem:
 
 #+REVEAL_HTML: <div style="text-align: center">
 $\textrm{argmin}_{\omega \in \Omega} (loss \circ nn (\omega; data))$
 #+REVEAL_HTML: </div>

* Gradient Descent

#+REVEAL_HTML: <div style="text-align: center">
$w' = w - \alpha * \frac {\partial f}{\partial w}$
#+REVEAL_HTML: </div>

| [[./pics/gradient2.png]] | [[./pics/loss_surface2.png]] |

* Automatic Differentiation

#+REVEAL_HTML: <div style="text-align: center">  
$nn = l_L \circ ... \circ l_1$ 

=> Ableitung ist: $Dnn = Dl_L \circ ... \circ Dl_1$
#+REVEAL_HTML: </div>

- Funktionskomposition ist assoziativ, d.h. wir können in bliebiger Reihenfolge
  auswerten

- Aufwand vorwärts (rechts-assoziativ) abhängig von Eingangsdimension (= Anzahl der
  Parameter), rückwärts (links-assoziativ) abhängig von Ausgangsdimension (= 1)

* Reverse Automatic Differentiation

$Dnn = Dl_L(l_{L-1}(...)) \circ ... \circ Dl_1(v)$

Um $Dl_i$ zu berechnen, müssen wir die Ausgabe von $l_{i - 1}$ kennen
  
  => "Wengert-Liste"

Deep Learning Bibliotheken sind im Wesentlichen Werkzeuge zur Generierung und 
Verwaltung von Berechnungsgraphen.

* Deep Learning mit TensorFlow

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 50%">
[[./pics/tf_graph.png]]
[[./pics/tensorboard-01.png]]
#+REVEAL_HTML: </div>

* Deep Learning mit TensorFlow

#+REVEAL_HTML: <div style="font-size: 70%;">
#+begin_src python

class SimpleNeuralNetwork:

    def __init__(self, dim):
        self.dims = [dim, dim, dim // 2, dim // 2, dim]
        self.weights = [] 
        self.biases = []
        for i in range(4):
            self.weights.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],self.dims[i])))
                )
            self.biases.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],1)))
                )
    
    def __call__(self, x):
        inputs = tf.convert_to_tensor([x], dtype=tf.float32)
        out = tf.matmul(self.weights[0], 
                        inputs, transpose_b=True) + self.biases[0]
        out = tf.tanh(out)
        out = tf.matmul(self.weights[1], out) + self.biases[1]
        out = tf.nn.relu(out)
        out = tf.matmul(self.weights[2], out) + self.biases[2]
        out = tf.tanh(out)
        
        return tf.matmul(self.weights[3], out) + self.biases[3]
#+end_src
#+REVEAL_HTML: </div>

* Deep Learning mit ConCat

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 450px; font-size: 100%">
#+begin_src haskell

simpleNeuralNetwork ::
  ( KnownNat n, 
    KnownNat m, 
    Additive numType, 
    Floating numType, 
    Ord numType
  ) =>
  SimpleNeuralNetworkPars f n m numType -> 
  f n numType -> 
  f n numType
simpleNeuralNetwork = 
  affine 
  @. affTanh 
  @. affRelu 
  @. affTanh

(@.) :: 
  (q s -> b -> c) -> 
  (p s -> a -> b) -> 
  ((q :*: p) s -> a -> c)
(g @. f) (q :*: p) = g q . f p

type p --* q = q :.: p

type Bump h = h :*: Par1

bump :: Num s => a s -> Bump a s
bump a = a :*: Par1 1

type a --+ b = Bump a --* b

type SimpleNeuralNetworkPars (f :: Nat -> * -> *) n m =
  ( (f m --+ f n)
      :*: (f m --+ f m)
      :*: (f n --+ f m)
      :*: (f n --+ f n)
  )

(<.>) :: (Foldable a, Zip a, Additive s, Num s) 
      => a s -> a s -> s
xs <.> ys = sumA (zipWith (*) xs ys)

linear :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --* b) s -> (a s -> b s)
linear (Comp1 ba) a = (<.> a) <$> ba

affine :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --+ b) s -> (a s -> b s)
affine m = linear m . bump

affRelu :: 
  ( Foldable a, 
    Zip a, 
    Functor b, 
    Ord s, 
    Additive s, 
    Num s
  ) => 
  (a --+ b) s -> (a s -> b s)
affRelu l = relus . affine l
#+end_src
#+REVEAL_HTML: </div>

* ConCat

[[./pics/concat.png]]

- Nutzt Isomorphie zwischen Lambda-Kalkülen und kartesisch abgeschlossenen Kategorien (CCC)
- Übersetzt Haskell-Core in kategorielle Sprache
- Abstrahiert Haskells Funktionspfeil (->)

* ConCat

#+begin_src haskell

magSqr :: Num a => (a, a) -> a
magSqr (a, b) = sqr a + sqr b
#+end_src

=> ConCat:

$magSqr = addC \circ (mulC \circ (exl \triangle exl) \triangle mulC \circ (exr \triangle exr))$

In Kategorie der Graphen ((a, a) `Graph` a):
#+ATTR_HTML: :height 300
[[./pics/magSqr.png]]

* Ableiten mit ConCat

Idee: ergänze Funktionen um ihre Ableitung

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto f(a) \Rightarrow a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

Kategorie der Generalisierten Ableitungen (Generalized Derivatives):
#+begin_src haskell

newtype GD k a b = D {unD :: a -> b :* (a `k` b)}  
#+end_src

* Ableiten mit ConCat

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance Category k => Category (GD k) where 
  ...
  D g . D f = 
    D (\ a -> 
          let (b, f') = f a  -- Kettenregel:
              (c, g') = g b  -- (g o f)'(x) = g'(f(x)) o f'(x)
           in (c, g' . f')
      )
#+end_src

* Ableiten mit ConCat

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance {-# overlappable #-} 
(LinearCat k s, Additive s, Num s) => NumCat (GD k) s where  
  ...
  -- Produktregel: (f(x) * g(x))' = f'(x)*g(x) + f(x)*g'(x)
  mulC    = D (mulC &&& \ (u,v) -> scale v |||| scale u)
#+end_src

* Forward Automatic Differentiation mit ConCat

#+CAPTION: magSqr in $GD (-+>)$
[[./pics/magSqr_D.png]]

* Reverse Automatic Differentiation mit ConCat

Duale Kategorie ("drehe Pfeile um")

#+REVEAL_HTML: <div style="text-align: center">
$a \rightarrow b \Rightarrow b \rightarrow a$
#+REVEAL_HTML: </div>

#+begin_src haskell
newtype Dual k a b = Dual (b `k` a)
#+end_src


* Reverse Automatic Differentiation mit ConCat

#+begin_src haskell

instance Category k => Category (Dual k) where
  ...
  (.) = inAbst2 (flip (.))

instance CoproductPCat k => ProductCat (Dual k) where
  ...
  -- exl :: (a, b) -> a; inlP :: a -> (a, b)
  exl = abst inlP
#+end_src

* Reverse Automatic Differentiation mit ConCat

#+ATTR_HTML: :width 500
#+CAPTION: magSqr in $GD (Dual(-+>))$
[[./pics/magSqr_dual.png]]

* Reverse Automatic Differentiation mit ConCat

#+begin_src haskell
type RAD = GD (Dual (-+>))

grad :: Num s => (a -> s) -> (a -> a)
grad = friemelOutGrad . toCcc @RAD

nnGrad :: paramType -> paramType
nnGrad = grad (loss . nn)
#+end_src

* Beschleunigtes Deep Learning in Haskell

"Data.Array.Accelerate defines an embedded array language for computations for 
high-performance computing in Haskell. ... These computations may then be online 
compiled and executed on a range of architectures."

Kategorie der Accelerate-Funktionen:
#+begin_src haskell
newtype AccFun a b where
  AccFun :: (AccValue a -> AccValue b) -> AccFun a b
#+end_src

#+begin_notes
(- native Haskell accelerate:-> AST)
- schreiben Berechnung als Haskell-Code, daraus entsteht accelerate AST
- accelerate AST wird zur Laufzeit von LLVM kompiliert
- kann dann gegen CPU/GPU/... ausgeführt werden
- Plugin erzeugt den acclerate AST
#+end_notes

* Beschleunigtes Deep Learning in Haskell

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 79%">
#+begin_src haskell
autoencoder ::
  forall m n.
  (KnownNat m, KnownNat n) => Autoencoder m n Double
autoencoder =
  affine @(Vector n) @. affTanh @(Vector n) @. affRelu @(Vector m) @. affTanh

autoencoderGrad ::
  forall m n.
  ( KnownNat m, KnownNat n) =>
  (Vector m Double, Vector m Double) ->
  AutoencoderPars m n Double ->
  AutoencoderPars m n Double
autoencoderGrad = errGrad autoencoder

autoencoderGradAccFun ::
  forall m n.
  ( KnownNat m, KnownNat n) =>
  (Vector m Double, Vector m Double) ->
  AutoencoderPars m n Double `AccFun` AutoencoderPars m n Double
autoencoderGradAccFun pair = AltCat.toCcc' (autoencoderGrad pair)
#+end_src
#+REVEAL_HTML: </div>
