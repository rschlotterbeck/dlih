#+title: Ist das ein Graph oder kann das weg? Funktionales Deep Learning in Haskell
#+author: Raoul Schlotterbeck
#+REVEAL_PLUGINS: (notes)
#+REVEAL_THEME: ./css/themes/active.css
#+REVEAL_HLEVEL: 100
#+REVEAL_TRANS: none
#+OPTIONS: toc:nil reveal-center:f H:4 num:nil
#+MACRO: inline-hs src_haskell[:exports code]{$1}

* Compiling to Categories

#+REVEAL_HTML: <div style="font-size: 80%"><p>
"/As so often, I find that talks that I'm giving are basically explaining what Conal 
Elliot and Ed Kmett have done several years ago./"
#+REVEAL_HTML: </p><figcaption style="font-size: 80%"> - Simon Peyton Jones </figcaption></div><img src="./pics/compiling_to_categories.png" style="height: 400px">

#+begin_notes

#+end_notes

* Siemens Anomaly App

#+REVEAL_HTML: <img style="border: 1px solid black" src="./pics/sap_small.png"> 

#+begin_notes
1:20 - 3:00


#+end_notes

* Neuronale Netze sind doch nur Funktionen

#+REVEAL_HTML: <div style="width: 100%; overflow: hidden;"> <div style="width: 550px; float: left; font-size: 90%">
#+begin_src haskell
neuralNet wL bL ... w1 b1 = 
  layer wL bL 
  . ... 
  . layer w1 b1

layer weightMatrix biasVector =
  fmap activationFunction
  . vectorAddition biasVector
  . matrixVectorProduct weightMatrix
#+end_src

#+REVEAL_HTML: </div><div style="margin-left: 550px;">
[[./pics/neural_net.png]]
#+REVEAL_HTML: </div><div style="text-align: center;">
$\Rightarrow$ Komposition purer Funktionen
#+REVEAL_HTML: </div></div>


#+begin_notes
3:00 - 6:30

- Gibt auch andere layers
#+end_notes

* Deep learning community: hold my beer

#+REVEAL_HTML: <p>
"[...] layers (die im modernen maschinellen Lernen als *zustandsbehaftete* Funktionen 
mit *impliziten Parametern* verstanden werden sollten) werden typischerweise als 
*Python-Klassen* dargestellt, deren Konstruktoren ihre Parameter *erzeugen und 
initialisieren* [...]"
#+REVEAL_HTML: </p>

#+REVEAL_HTML: <div style="font-size: 35%">
Übersetzt aus:
#+REVEAL_HTML: <a href="https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf"> Paszke, Adam, et al. "Pytorch: An imperative style, high-performance deep learning library." Advances in neural information processing systems 32 (2019)</a> </div>

#+begin_notes
6:30 - 8:40

#+end_notes

* Trainingsalgorithmus

- Finde Parameter, für die das NN auf gegebenem Trainingsdatensatz
  möglichst gute Ergebnisse liefert

- Güte durch skalarwertige Fehlerfunktion beurteilt

 $\Rightarrow$ Löse Optimierungsproblem:
 
 #+REVEAL_HTML: <div style="text-align: center">
 $\textrm{argmin}_{\omega \in \Omega} (\textrm{loss} \circ \textrm{neuralNet} (\omega; \textrm{data}))$
 #+REVEAL_HTML: </div>

#+begin_notes
8:40 - 9:50


#+end_notes

* Gradient Descent

#+REVEAL_HTML: <div style="text-align: center; font-size:90%">
$w_{n+1} = w_n - \alpha \cdot \frac {\partial f}{\partial w}$
#+REVEAL_HTML: </div><hr><div style="width: 100%; height: 350px; overflow: hidden;"> <div style="width: 450px; float: left"><img src="./pics/gradient2.png" style="width: 450px; height: 300px"></div>
#+REVEAL_HTML: <div style="margin-left: 450px;"><img src="./pics/loss_surface3.gif" style="width: 500px; height: 350px"></div></div><hr>
#+REVEAL_HTML: <div style="font-size: 35%"> Abbildungen von: <a> http://citadel.sjfc.edu/faculty/kgreen/vector/Block2/pder/node8.html </a>, <a>https://upload.wikimedia.org/wikipedia/commons/a/a3/Gradient_descent.gif </a></div>

#+begin_notes
9:50 - 12:00

#+end_notes

* Automatic Differentiation

#+REVEAL_HTML: <div style="text-align: center">  
$nn = l_L \circ ... \circ l_1$ 

$\Rightarrow$ Ableitung: $Dnn = Dl_L \circ ... \circ Dl_1$
#+REVEAL_HTML: </div>

- Funktionskomposition ist assoziativ, erlaubt Auswertung in beliebiger Reihenfolge

- Aufwand "vorwärts" abhängig von Eingangsdimension (hier: Anzahl der
  Parameter; heute bis $10^{11}$)

- Aufwand "rückwärts" abhängig von Ausgangsdimension (hier: 1)

#+begin_notes
12:00 - 13:50


#+end_notes

* Reverse Automatic Differentiation

$Dnn(v) = Dl_L(l_{L-1}(...)) \circ ... \circ Dl_1(v)$

Um $Dl_i$ zu berechnen, müssen wir die Ausgabe von $l_{i - 1}$ kennen
  
  $\Rightarrow$ "Wengert-Liste"

Deep Learning Bibliotheken sind im Wesentlichen Werkzeuge zur Generierung und 
Verwaltung von Berechnungsgraphen.

#+begin_notes
13:50 - 15:15

#+end_notes

* TensorFlow Graphen

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 50%">
[[./pics/tf_graph.png]]
[[./pics/tensorboard-01.png]]
#+REVEAL_HTML: </div>

#+begin_notes
15:15 - 17:00

#+end_notes

* Implementierung in TensorFlow

#+REVEAL_HTML: <div style="font-size: 70%;">
#+begin_src python

class SimpleNN:

    def __init__(self, dimIn dimOut):
        self.dims = [dimIn, dimIn, dimIn, dimOut, dimOut]
        self.weights = [] 
        self.biases = []
        for i in range(4):
            self.weights.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],self.dims[i])))
                )
            self.biases.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],1)))
                )
    
    def __call__(self, x):
        inputs = tf.convert_to_tensor([x], dtype=tf.float32)
        out = tf.matmul(self.weights[0], 
                        inputs, transpose_b=True) + self.biases[0]
        out = tf.tanh(out)
        out = tf.matmul(self.weights[1], out) + self.biases[1]
        out = tf.nn.relu(out)
        out = tf.matmul(self.weights[2], out) + self.biases[2]
        out = tf.tanh(out)
        
        return tf.matmul(self.weights[3], out) + self.biases[3]
#+end_src
#+REVEAL_HTML: </div>

#+begin_notes
17:00 - 20:30

- Graphencode
- Typen spezialisiert
- lässt sich nicht Generalisieren...
- ... testen
- Pythonintegration
- dimensionsfehler
#+end_notes

* Neuronale Netze mit ConCat

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 450px; font-size: 90%">
#+begin_src haskell

simpleNN ::
  ( KnownNat m,
    KnownNat n,
    Functor f,
    Foldable f, 
    Floating num,
    ...
  ) =>
  SimpleNNParameters f m n num -> 
  f m num -> 
  f n num
simpleNN = 
  affine 
  @. affTanh 
  @. affRelu 
  @. affTanh

(@.) :: 
  (q s -> b -> c) -> 
  (p s -> a -> b) -> 
  ((q :*: p) s -> a -> c)
(g @. f) (q :*: p) = g q . f p

type p --* q = q :.: p

type Bump h = h :*: Par1

bump :: Num s => a s -> Bump a s
bump a = a :*: Par1 1

type a --+ b = Bump a --* b

type SimpleNNParameters (f :: Nat -> * -> *) m n =
  ( (f n --+ f n)
      :*: (f m --+ f n)
      :*: (f m --+ f m)
      :*: (f m --+ f m)
  )

(<.>) :: (Foldable a, Zip a, Additive s, Num s) 
      => a s -> a s -> s
xs <.> ys = sumA (zipWith (*) xs ys)

linear :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --* b) s -> (a s -> b s)
linear (Comp1 ba) a = (<.> a) <$> ba

affine :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --+ b) s -> (a s -> b s)
affine m = linear m . bump

affRelu :: 
  ( Foldable a, 
    Zip a, 
    Functor b, 
    Ord s, 
    Additive s, 
    Num s
  ) => 
  (a --+ b) s -> (a s -> b s)
affRelu l = relus . affine l
#+end_src
#+REVEAL_HTML: </div>

#+begin_notes
20:30 - 22:00

- pure Funktion
- exakte Rep eines NNs
- generische Typen
- Dimensionalität
- testen
#+end_notes

* ConCat Funktionsweise

#+REVEAL_HTML: <img src="./pics/concat-pipeline.png" style="width: 90%">

- Nutzt Isomorphie zwischen Lambda-Kalkülen und kartesisch abgeschlossenen Kategorien (CCC) [*]
- Übersetzt Haskell-Core in kategorielle Sprache
- Ausdrücke in kategorieller Sprache können in beliebigen CCCs interpretiert werden
- Abstrahiert dadurch Haskells Funktionspfeil {{{inline-hs((->))}}}

#+REVEAL_HTML: <div style="font-size: 35%; padding: 100px"> [*]  <a href="https://dl.acm.org/doi/10.5555/645691.665261"> Joachim Lambek "Cartesian Closed Categories and Typed Lambda-calculi", In 13th Spring School on Combinators and Functional Programming Languages, 1985 </a></div>

#+begin_notes
22:00 - 22:40


#+end_notes

* Beispiel einer ConCat-Transformation

#+begin_src haskell

magSqr :: Num a => (a, a) -> a
magSqr (a, b) = sqr a + sqr b
#+end_src

$\Rightarrow$ ConCat:

$magSqr = addC \circ (mulC \circ (exl \triangle exl) \triangle mulC \circ (exr \triangle exr))$

In Kategorie der Graphen -- src_haskell[:exports code]{(a, a) `Graph` a}:
#+REVEAL_HTML: <div style="text-align: center"><img src="./pics/magSqr.png" height="250px%"><div style="font-size:35%"> Grafik von:
#+REVEAL_HTML: <a href="https://arxiv.org/abs/1804.00746">Conal Elliot "The Simple Essence of Automatic Differentiation", ICFP 2018 </a></div></div>

#+begin_notes
22:40 - 25:20


#+end_notes

* Generalized Derivatives

Idee: Ergänze Funktion um ihre Ableitung

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto f(a) \Rightarrow a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

Kategorie der generalisierten Ableitungen:
#+begin_src haskell

newtype GD k a b = D {unD :: a -> b :* (a `k` b)}  
#+end_src

#+begin_notes
25:20 - 26:30


#+end_notes

* Komposition für Generalized Derivatives

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance Category k => Category (GD k) where 
  ...
  D g . D f = 
    D (\ a -> 
          let (b, f') = f a
              (c, g') = g b
           in (c, g' . f')
      )
#+end_src

#+REVEAL_HTML: <div style="text-align: center; margin-top: 50px">
Kettenregel: $(g \circ f)'(x) = g'(f(x)) \circ f'(x)$
#+REVEAL_HTML: </div>

#+begin_notes
26:30 - 27:20


#+end_notes

* Multiplikation für Generalized Derivatives

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance (LinearCat k s, Additive s, Num s) => NumCat (GD k) s where  
  ...
  mulC    = D (mulC &&& \ (u,v) -> scale v |||| scale u)
#+end_src

#+REVEAL_HTML: <div style="text-align: center; margin-top: 50px">
Produktregel: $(f(x) \cdot g(x))' = f'(x) \cdot g(x) + f(x) \cdot g'(x)$
#+REVEAL_HTML: </div>

#+begin_notes
27:20 - 28:00

#+end_notes

* Forward Automatic Differentiation

#+REVEAL_HTML: <div style="text-align: center">
#+CAPTION: =magSqr in GD (-+>)=
[[./pics/magSqr_D.png]]

#+begin_notes
28:00 - 29:15

#+end_notes

* Duale Kategorien

Im Dual einer Kategorie drehen sich alle Pfeile um

#+REVEAL_HTML: <div style="text-align: center">
$a \rightarrow b \Rightarrow b \rightarrow a$
#+REVEAL_HTML: </div>

In Haskell:
#+begin_src haskell
newtype Dual k a b = Dual (b `k` a)
#+end_src

#+begin_notes
29:15 - 30:00

#+end_notes

* Beispiele Dualer Morphismen

#+begin_src haskell


instance Category k => Category (Dual k) where
  ...
  -- flip :: (a -> b -> c) -> b -> a -> c
  (.) = inAbst2 (flip (.))

instance CoproductPCat k => ProductCat (Dual k) where
  ...
  -- exl :: (a, b) -> a; inlP :: a -> (a, b)
  exl = abst inlP
#+end_src

#+begin_notes
30:00 - 31:00

#+end_notes

* Reverse Automatic Differentiation

#+REVEAL_HTML: <div style="text-align: center">
#+ATTR_HTML: :width 500
#+CAPTION: =magSqr in GD (Dual(-+>))=
[[./pics/magSqr_dual.png]]

#+begin_notes
31:00 - 31:20

#+end_notes

* Graphenfreie Gradienten

#+begin_src haskell
type RAD = GD (Dual (-+>))

grad :: Num s => (a -> s) -> (a -> a)
grad = friemelOutGrad . toCcc @RAD

nnGrad :: parameters -> parameters
nnGrad = grad (loss . nn)
#+end_src

#+begin_notes
31:20 - 34:30

#+end_notes

* Beschleunigtes Deep Learning in Haskell

#+REVEAL_HTML: <blockquote>
"Data.Array.Accelerate defines an embedded array language for computations for 
high-performance computing in Haskell. [...] These computations may then be online 
compiled and executed on a range of architectures."
#+REVEAL_HTML: </blockquote>

Kategorie der Accelerate-Funktionen:
#+begin_src haskell
newtype AccFun a b where
  AccFun :: (AccValue a -> AccValue b) -> AccFun a b
#+end_src

#+begin_notes
34:30 - 35:30

(- native Haskell accelerate:-> AST)
- schreiben Berechnung als Haskell-Code, daraus entsteht accelerate AST
- accelerate AST wird zur Laufzeit von LLVM kompiliert
- kann dann gegen CPU/GPU/... ausgeführt werden
- Plugin erzeugt den acclerate AST
#+end_notes

* ConCelerate: ConCat + Accelerate

#+REVEAL_HTML: <div>
#+begin_src haskell
simpleNN :: (SimpleNNConstraints f m n num) => SimpleNN f m n num
simpleNN = affine @. affTanh @. affRelu @. affTanh

simpleNNGrad ::
  (KnownNat m, KnownNat n) =>
  (Vector m Double, Vector n Double) ->
  SimpleNNParameters m n Double ->
  SimpleNNParameters m n Double
simpleNNGrad = errGrad simpleNN

simpleNNGradAccFun ::
  (KnownNat m, KnownNat n) =>
  (Vector m Double, Vector n Double) ->
  SimpleNNParameters m n Double `AccFun` SimpleNNParameters m n Double
simpleNNGradAccFun pair = toCcc (simpleNNGrad pair)
#+end_src
#+REVEAL_HTML: </div>

#+begin_notes
35:30

#+end_notes

* Vielen Dank!

#+REVEAL_HTML: <div style="width: 50%; overflow: hidden; margin-left: 100px"> <div style="width: 100px; float: left">
[[./pics/concat_qr.png]]
#+REVEAL_HTML: </div><div style="margin-left: 150px; padding: 65px"> ConCat </div><hr>
#+REVEAL_HTML: <div style="width: 100px; float: left">
[[./pics/accelerate_qr.png]]
#+REVEAL_HTML: </div><div style="margin-left: 150px; padding: 65px"> Accelerate </div><hr>
#+REVEAL_HTML: <div style="width: 100px; float: left">
[[./pics/ag_qr.png]]
#+REVEAL_HTML: </div><div style="margin-left: 150px; padding: 65px"> Active Group </div></div>