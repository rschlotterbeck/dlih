#+title: Ist das ein Graph oder kann das weg? Funktionales Deep Learning in Haskell
#+author: Raoul Schlotterbeck
#+REVEAL_PLUGINS: (notes)
#+REVEAL_THEME: ./css/themes/active.css
#+REVEAL_HLEVEL: 100
#+REVEAL_TRANS: none
#+OPTIONS: toc:nil reveal-center:f H:4

* Neuronale Netze

#+REVEAL_HTML: <div style="width: 100%; overflow: hidden;">

#+REVEAL_HTML: <div style="width: 500px; float: left;">
#+begin_src haskell
neuralNet wL bL ... w1 b1 = 
  layer wL bL 
  . ... 
  . layer w1 b1

layer w b =
  fmap activationFunction
  . vectorAddition b
  . matrixVectorProduct w
#+end_src

#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="margin-left: 500px;">
[[./pics/neural_net.jpg]]
#+REVEAL_HTML: </div>

#+REVEAL_HTML: <div style="text-align: center;">
=> Komposition purer Funktionen
#+REVEAL_HTML: </div>

#+REVEAL_HTML: </div>


#+begin_notes
foo
#+end_notes

* Neuronale Netze

#+REVEAL_HTML: <p>
"... Schichten (die im modernen maschinellen Lernen als zustandsbehaftete Funktionen 
mit impliziten Parametern verstanden werden sollten) werden typischerweise als 
Python-Klassen dargestellt, deren Konstruktoren ihre Parameter erzeugen und 
initialisieren..."
#+REVEAL_HTML: </p>

#+REVEAL_HTML: <div style="font-size: 60%;">
#+REVEAL_HTML: <p>
"... layers (which in modern machine learning should really be understood as 
stateful functions with implicit parameters) are typically expressed as Python 
classes whose constructors create and initialize their parameters..."
#+REVEAL_HTML: </p>
#+REVEAL_HTML: <cite style:"font-size=30%">
https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf
#+REVEAL_HTML: </cite>
#+REVEAL_HTML: </div>

* Training

- Finde Parameter $\omega_i = (w_i, b_i)$, für die das NN auf einem gegebenen Trainingsdatensatz
  möglichst gute Ergebnisse liefert

- Was gut ist, wird von einer (skalarwertigen) Fehlerfunktion beurteilt

 => Löse Optimierungsproblem:
 
 #+REVEAL_HTML: <div style="text-align: center">
 $\textrm{argmin}_{\omega \in \Omega} (loss \circ nn (\omega; data))$
 #+REVEAL_HTML: </div>

* Gradient Descent

#+REVEAL_HTML: <div style="text-align: center">
$w' = w - \alpha * \frac {\partial f}{\partial w}$
#+REVEAL_HTML: </div>

| [[./pics/gradient2.png]] | [[./pics/loss_surface2.png]] |

* Automatic Differentiation

#+REVEAL_HTML: <div style="text-align: center">  
$nn = l_L \circ ... \circ l_1$ 

=> Ableitung ist: $Dnn = Dl_L \circ ... \circ Dl_1$
#+REVEAL_HTML: </div>

- Funktionskomposition ist assoziativ, d.h. wir können in bliebiger Reihenfolge
  auswerten

- Aufwand vorwärts (rechts-assoziativ) abhängig von Eingangsdimension (= Anzahl der
  Parameter), rückwärts (links-assoziativ) abhängig von Ausgangsdimension (= 1)

* Reverse Automatic Differentiation

$Dnn = Dl_L(l_{L-1}(...)) \circ ... \circ Dl_1(v)$

Um $Dl_i$ zu berechnen, müssen wir die Ausgabe von $l_{i - 1}$ kennen
  
  => "Wengert-Liste"

Deep Learning Bibliotheken sind im Wesentlichen Werkzeuge zur Generierung und 
Verwaltung von Berechnungsgraphen.

* Deep Learning mit TensorFlow

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 50%">
[[./pics/tf_graph.png]]
[[./pics/tensorboard-01.png]]
#+REVEAL_HTML: </div>

* Deep Learning mit TensorFlow

#+REVEAL_HTML: <div style="font-size: 70%;">
#+begin_src python

class SimpleNeuralNetwork:

    def __init__(self, dim):
        self.dims = [dim, dim, dim // 2, dim // 2, dim]
        self.weights = [] 
        self.biases = []
        for i in range(4):
            self.weights.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],self.dims[i])))
                )
            self.biases.append(
                tf.Variable(tf.random.normal(shape=(self.dims[i+1],1)))
                )
    
    def __call__(self, x):
        inputs = tf.convert_to_tensor([x], dtype=tf.float32)
        out = tf.matmul(self.weights[0], 
                        inputs, transpose_b=True) + self.biases[0]
        out = tf.tanh(out)
        out = tf.matmul(self.weights[1], out) + self.biases[1]
        out = tf.nn.relu(out)
        out = tf.matmul(self.weights[2], out) + self.biases[2]
        out = tf.tanh(out)
        
        return tf.matmul(self.weights[3], out) + self.biases[3]
#+end_src
#+REVEAL_HTML: </div>

* Deep Learning mit ConCat

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 450px; font-size: 100%">
#+begin_src haskell

simpleNeuralNetwork ::
  ( KnownNat n, 
    KnownNat m, 
    Additive numType, 
    Floating numType, 
    Ord numType
  ) =>
  SimpleNeuralNetworkPars f n m numType -> 
  f n numType -> 
  f n numType
simpleNeuralNetwork = 
  affine 
  @. affTanh 
  @. affRelu 
  @. affTanh

(@.) :: 
  (q s -> b -> c) -> 
  (p s -> a -> b) -> 
  ((q :*: p) s -> a -> c)
(g @. f) (q :*: p) = g q . f p

type p --* q = q :.: p

type Bump h = h :*: Par1

bump :: Num s => a s -> Bump a s
bump a = a :*: Par1 1

type a --+ b = Bump a --* b

type SimpleNeuralNetworkPars (f :: Nat -> * -> *) n m =
  ( (f m --+ f n)
      :*: (f m --+ f m)
      :*: (f n --+ f m)
      :*: (f n --+ f n)
  )

(<.>) :: (Foldable a, Zip a, Additive s, Num s) 
      => a s -> a s -> s
xs <.> ys = sumA (zipWith (*) xs ys)

linear :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --* b) s -> (a s -> b s)
linear (Comp1 ba) a = (<.> a) <$> ba

affine :: (Foldable a, Zip a, Functor b, Additive s, Num s)
       => (a --+ b) s -> (a s -> b s)
affine m = linear m . bump

affRelu :: 
  ( Foldable a, 
    Zip a, 
    Functor b, 
    Ord s, 
    Additive s, 
    Num s
  ) => 
  (a --+ b) s -> (a s -> b s)
affRelu l = relus . affine l
#+end_src
#+REVEAL_HTML: </div>

* ConCat

[[./pics/concat.png]]

- Nutzt Isomorphie zwischen Lambda-Kalkülen und kartesisch abgeschlossenen Kategorien (CCC)
- Übersetzt Haskell-Core in kategorielle Sprache
- Abstrahiert Haskells Funktionspfeil (->)

* ConCat

#+begin_src haskell

magSqr :: Num a => (a, a) -> a
magSqr (a, b) = sqr a + sqr b
#+end_src

=> ConCat:

$magSqr = addC \circ (mulC \circ (exl \triangle exl) \triangle mulC \circ (exr \triangle exr))$

In Kategorie der Graphen ((a, a) `Graph` a):
#+ATTR_HTML: :height 300
[[./pics/magSqr.png]]

* Ableiten mit ConCat

Idee: ergänze Funktionen um ihre Ableitung

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto f(a) \Rightarrow a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

Kategorie der Generalisierten Ableitungen (Generalized Derivatives):
#+begin_src haskell

newtype GD k a b = D {unD :: a -> b :* (a `k` b)}  
#+end_src

* Ableiten mit ConCat

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance Category k => Category (GD k) where 
  ...
  D g . D f = 
    D (\ a -> 
          let (b, f') = f a  -- Kettenregel:
              (c, g') = g b  -- (g o f)'(x) = g'(f(x)) o f'(x)
           in (c, g' . f')
      )
#+end_src

* Ableiten mit ConCat

#+REVEAL_HTML: <div style="text-align: center;">
$a \mapsto (f(a), f'(a))$
#+REVEAL_HTML: </div>

#+begin_src haskell
instance {-# overlappable #-} 
(LinearCat k s, Additive s, Num s) => NumCat (GD k) s where  
  ...
  -- Produktregel: (f(x) * g(x))' = f'(x)*g(x) + f(x)*g'(x)
  mulC    = D (mulC &&& \ (u,v) -> scale v |||| scale u)
#+end_src

* Forward Automatic Differentiation mit ConCat

#+CAPTION: magSqr in $GD (-+>)$
[[./pics/magSqr_D.png]]

* Reverse Automatic Differentiation mit ConCat

Duale Kategorie ("drehe Pfeile um")

#+REVEAL_HTML: <div style="text-align: center">
$a \rightarrow b \Rightarrow b \rightarrow a$
#+REVEAL_HTML: </div>

#+begin_src haskell
newtype Dual k a b = Dual (b `k` a)
#+end_src


* Reverse Automatic Differentiation mit ConCat

#+begin_src haskell

instance Category k => Category (Dual k) where
  ...
  (.) = inAbst2 (flip (.))

instance CoproductPCat k => ProductCat (Dual k) where
  ...
  -- exl :: (a, b) -> a; inlP :: a -> (a, b)
  exl = abst inlP
#+end_src

* Reverse Automatic Differentiation mit ConCat

#+ATTR_HTML: :width 500
#+CAPTION: magSqr in $GD (Dual(-+>))$
[[./pics/magSqr_dual.png]]

* Reverse Automatic Differentiation mit ConCat

#+begin_src haskell
type RAD = GD (Dual (-+>))

grad :: Num s => (a -> s) -> (a -> a)
grad = friemelOutGrad . toCcc @RAD

nnGrad :: paramType -> paramType
nnGrad = grad (loss . nn)
#+end_src

* Adam Optimizer in TensorFlow

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 50%">
#+begin_src python
class Optimizer(_BaseOptimizer):

    def __init__(
        self,
        name,
        weight_decay=0,
        clipnorm=None,
        clipvalue=None,
        global_clipnorm=None,
        use_ema=False,
        ema_momentum=0.99,
        ema_overwrite_frequency=None,
        jit_compile=True,
        **kwargs,
    ):
        """Create a new Optimizer."""

        super().__init__(
            name,
            weight_decay,
            clipnorm,
            clipvalue,
            global_clipnorm,
            use_ema,
            ema_momentum,
            ema_overwrite_frequency,
            jit_compile,
            **kwargs,
        )
        self._distribution_strategy = tf.distribute.get_strategy()

    def add_variable_from_reference(
        self, model_variable, variable_name, shape=None, initial_value=None
    ):
        strategy = tf.distribute.get_strategy()
        with strategy.extended.colocate_vars_with(model_variable):
            return super().add_variable_from_reference(
                model_variable, variable_name, shape, initial_value
            )

    def _var_key(self, variable):
        """Get a unique identifier of the given variable."""

        # Get the distributed variable if it exists.
        # TODO(b/197554203): replace _distributed_container() with a public api.
        if hasattr(variable, "_distributed_container"):
            variable = variable._distributed_container()
        elif (
            tf_utils.is_extension_type(variable)
            and hasattr(variable, "handle")
            and hasattr(variable.handle, "_distributed_container")
        ):
            # For ResourceVariables, the _distributed_container attribute
            # is added to their handle tensors.
            variable = variable.handle._distributed_container()
        return super()._var_key(variable)

    def aggregate_gradients(self, grads_and_vars):
        """Aggregate gradients on all devices.
        By default we will perform reduce_sum of gradients across devices. Users
        can implement their own aggregation logic by overriding this method.
        Args:
          grads_and_vars: List of (gradient, variable) pairs.
        Returns:
          List of (gradient, variable) pairs.
        """
        return optimizer_utils.all_reduce_sum_gradients(grads_and_vars)

    def apply_gradients(
        self,
        grads_and_vars,
        name=None,
        skip_gradients_aggregation=False,
        **kwargs,
    ):
        """Apply gradients to variables.
        Args:
          grads_and_vars: List of `(gradient, variable)` pairs.
          name: string, defaults to None. The name of the namescope to
            use when creating variables. If None, `self.name` will be used.
          skip_gradients_aggregation: If true, gradients aggregation will not be
            performed inside optimizer. Usually this arg is set to True when you
            write custom code aggregating gradients outside the optimizer.
          **kwargs: keyword arguments only used for backward compatibility.
        Returns:
          A `tf.Variable`, representing the current iteration.
        Raises:
          TypeError: If `grads_and_vars` is malformed.
          RuntimeError: If called in a cross-replica context.
        """
        # `experimental_aggregate_gradients` is an arg in `apply_gradients` of
        # v2 optimizer -- the reverse of `skip_gradients_aggregation`.
        # We read it from kwargs for backward compatibility.
        experimental_aggregate_gradients = kwargs.pop(
            "experimental_aggregate_gradients", True
        )
        if not skip_gradients_aggregation and experimental_aggregate_gradients:
            grads_and_vars = self.aggregate_gradients(grads_and_vars)
        return super().apply_gradients(grads_and_vars, name=name)

    def _apply_weight_decay(self, variables):
        # Apply weight decay in distributed setup.
        if self.weight_decay is None:
            return

        def distributed_apply_weight_decay(distribution, variables, **kwargs):
            def weight_decay_fn(variable):
                if self._use_weight_decay(variable):
                    lr = tf.cast(self.learning_rate, variable.dtype)
                    wd = tf.cast(self.weight_decay, variable.dtype)
                    variable.assign_sub(variable * wd * lr)

            for variable in variables:
                distribution.extended.update(
                    variable, weight_decay_fn, group=False
                )

        tf.__internal__.distribute.interim.maybe_merge_call(
            distributed_apply_weight_decay,
            self._distribution_strategy,
            variables,
        )

    def _internal_apply_gradients(self, grads_and_vars):
        return tf.__internal__.distribute.interim.maybe_merge_call(
            self._distributed_apply_gradients_fn,
            self._distribution_strategy,
            grads_and_vars,
        )

    def _overwrite_model_variables_with_average_value_helper(self, var_list):
        """Helper function to _overwrite_model_variables_with_average_value.
        This function overwrites variables on each device.
        Args:
          var_list: list of model variables.
        """
        strategy = self._distribution_strategy
        # Override model variable by the stored average value on all devices.
        for var, average_var in zip(
            var_list, self._model_variables_moving_average
        ):
            strategy.extended.update(
                var, lambda a, b: a.assign(b), args=(average_var,)
            )

    def _update_model_variables_moving_average(self, var_list):
        """Update the stored moving average using the latest value."""
        if self.use_ema:

            def update_average(average, var):
                average.assign(
                    self.ema_momentum * average + (1 - self.ema_momentum) * var
                )

            for var, average in zip(
                var_list, self._model_variables_moving_average
            ):
                self._distribution_strategy.extended.update(
                    average, update_average, args=(var,), group=False
                )

    def _distributed_apply_gradients_fn(
        self, distribution, grads_and_vars, **kwargs
    ):
        """`apply_gradients` using a `DistributionStrategy`."""

        def apply_grad_to_update_var(var, grad):
            if self.jit_compile:
                return self._update_step_xla(grad, var, id(self._var_key(var)))
            else:
                return self._update_step(grad, var)

        for grad, var in grads_and_vars:
            distribution.extended.update(
                var, apply_grad_to_update_var, args=(grad,), group=False
            )

        if self.use_ema:
            _, var_list = zip(*grads_and_vars)
            self._update_model_variables_moving_average(var_list)
            if self.ema_overwrite_frequency:
                # Only when self.ema_overwrite_frequency is not None, we
                # overwrite the model variables.
                should_overwrite_model_vars = (
                    self.iterations + 1
                ) % self.ema_overwrite_frequency == 0
                tf.cond(
                    tf.cast(should_overwrite_model_vars, tf.bool),
                    true_fn=lambda: self._overwrite_model_variables_with_average_value(  # noqa: E501
                        var_list
                    ),
                    false_fn=lambda: None,
                )
        return self.iterations.assign_add(1)

class Adam(optimizer.Optimizer):
    def __init__(
        self,
        learning_rate=0.001,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-7,
        amsgrad=False,
        weight_decay=None,
        clipnorm=None,
        clipvalue=None,
        global_clipnorm=None,
        use_ema=False,
        ema_momentum=0.99,
        ema_overwrite_frequency=None,
        jit_compile=True,
        name="Adam",
        **kwargs
    ):
        super().__init__(
            name=name,
            weight_decay=weight_decay,
            clipnorm=clipnorm,
            clipvalue=clipvalue,
            global_clipnorm=global_clipnorm,
            use_ema=use_ema,
            ema_momentum=ema_momentum,
            ema_overwrite_frequency=ema_overwrite_frequency,
            jit_compile=jit_compile,
            **kwargs
        )
        self._learning_rate = self._build_learning_rate(learning_rate)
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.amsgrad = amsgrad

    def build(self, var_list):
        """Initialize optimizer variables.
        Adam optimizer has 3 types of variables: momentums, velocities and
        velocity_hat (only set when amsgrad is applied),
        Args:
          var_list: list of model variables to build Adam variables on.
        """
        super().build(var_list)
        if hasattr(self, "_built") and self._built:
            return
        self._built = True
        self._momentums = []
        self._velocities = []
        for var in var_list:
            self._momentums.append(
                self.add_variable_from_reference(
                    model_variable=var, variable_name="m"
                )
            )
            self._velocities.append(
                self.add_variable_from_reference(
                    model_variable=var, variable_name="v"
                )
            )
        if self.amsgrad:
            self._velocity_hats = []
            for var in var_list:
                self._velocity_hats.append(
                    self.add_variable_from_reference(
                        model_variable=var, variable_name="vhat"
                    )
                )

    def update_step(self, gradient, variable):
        """Update step given gradient and the associated model variable."""
        beta_1_power = None
        beta_2_power = None
        lr = tf.cast(self.learning_rate, variable.dtype)
        local_step = tf.cast(self.iterations + 1, variable.dtype)
        beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)
        beta_2_power = tf.pow(tf.cast(self.beta_2, variable.dtype), local_step)

        var_key = self._var_key(variable)
        m = self._momentums[self._index_dict[var_key]]
        v = self._velocities[self._index_dict[var_key]]

        alpha = lr * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)

        if isinstance(gradient, tf.IndexedSlices):
            # Sparse gradients.
            m.assign_add(-m * (1 - self.beta_1))
            m.scatter_add(
                tf.IndexedSlices(
                    gradient.values * (1 - self.beta_1), gradient.indices
                )
            )
            v.assign_add(-v * (1 - self.beta_2))
            v.scatter_add(
                tf.IndexedSlices(
                    tf.square(gradient.values) * (1 - self.beta_2),
                    gradient.indices,
                )
            )
            if self.amsgrad:
                v_hat = self._velocity_hats[self._index_dict[var_key]]
                v_hat.assign(tf.maximum(v_hat, v))
                v = v_hat
            variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))
        else:
            # Dense gradients.
            m.assign_add((gradient - m) * (1 - self.beta_1))
            v.assign_add((tf.square(gradient) - v) * (1 - self.beta_2))
            if self.amsgrad:
                v_hat = self._velocity_hats[self._index_dict[var_key]]
                v_hat.assign(tf.maximum(v_hat, v))
                v = v_hat
            variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))

    def get_config(self):
        config = super().get_config()

        config.update(
            {
                "learning_rate": self._serialize_hyperparameter(
                    self._learning_rate
                ),
                "beta_1": self.beta_1,
                "beta_2": self.beta_2,
                "epsilon": self.epsilon,
                "amsgrad": self.amsgrad,
            }
        )
        return config
#+end_src
#+REVEAL_HTML: </div>

* Adam Optimizer mit ConCat

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 50%">
#+begin_src haskell

adamStep ::
  forall numType paramStructure.
  ( Functor paramStructure
  , Zip.Zip paramStructure
  , Additive1 paramStructure
  , Additive numType
  , Num numType
  , Fractional numType
  , Floating numType
  , Additive (paramStructure numType)
  ) =>
  AdamParameters numType ->
  (paramStructure numType -> paramStructure numType) ->
  Unop (AdamState (paramStructure numType))
adamStep
  (beta1, beta2, epsilon, alpha)
  calcGrad
  (params, fstMEstimate, sndMEstimate, stepCount) =
    (params', fstMEstimate', sndMEstimate', stepCount')
    where
      stepCount' = stepCount + 1
      stepCount'' = fromIntegral stepCount'
      grad = calcGrad params
      fstMEstimate' =
        beta1 *^ fstMEstimate ^+^ (1 - beta1) *^ grad
          <+ additive1 @paramStructure @numType
      sndMEstimate' =
        beta2 *^ sndMEstimate
          ^+^ (1 - beta2) *^ Zip.zipWith (*) grad grad
          <+ additive1 @paramStructure @numType
      fstMEstimateHat = fstMEstimate' ^/ (1 - beta1 ** stepCount'')
      sndMEstimateHat = sndMEstimate' ^/ (1 - beta2 ** stepCount'')
      sqrSndMEstimateHat = sqrt <$> sndMEstimateHat
      effectiveGradient =
        Zip.zipWith (/) fstMEstimateHat ((+ epsilon) <$> sqrSndMEstimateHat)
      params' = params ^-^ alpha *^ effectiveGradient
#+end_src
#+REVEAL_HTML: </div>

* Beschleunigtes Deep Learning in Haskell

"Data.Array.Accelerate defines an embedded array language for computations for 
high-performance computing in Haskell. ... These computations may then be online 
compiled and executed on a range of architectures."

Kategorie der Accelerate-Funktionen:
#+begin_src haskell
newtype AccFun a b where
  AccFun :: (AccValue a -> AccValue b) -> AccFun a b
#+end_src

#+begin_notes
(- native Haskell accelerate:-> AST)
- schreiben Berechnung als Haskell-Code, daraus entsteht accelerate AST
- accelerate AST wird zur Laufzeit von LLVM kompiliert
- kann dann gegen CPU/GPU/... ausgeführt werden
- Plugin erzeugt den acclerate AST
#+end_notes

* Beschleunigtes Deep Learning in Haskell

#+REVEAL_HTML: <div style="overflow-x: hidden; overflow-y: auto; height: 500px; font-size: 79%">
#+begin_src haskell
autoencoder ::
  forall m n.
  (KnownNat m, KnownNat n) => Autoencoder m n Double
autoencoder =
  affine @(Vector n) @. affTanh @(Vector n) @. affRelu @(Vector m) @. affTanh

autoencoderGrad ::
  forall m n.
  ( KnownNat m, KnownNat n) =>
  (Vector m Double, Vector m Double) ->
  AutoencoderPars m n Double ->
  AutoencoderPars m n Double
autoencoderGrad = errGrad autoencoder

autoencoderGradAccFun ::
  forall m n.
  ( KnownNat m, KnownNat n) =>
  (Vector m Double, Vector m Double) ->
  AutoencoderPars m n Double `AccFun` AutoencoderPars m n Double
autoencoderGradAccFun pair = AltCat.toCcc' (autoencoderGrad pair)
#+end_src
#+REVEAL_HTML: </div>
